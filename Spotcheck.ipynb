{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wooihaw/ERA3036_T2310/blob/main/Tutorial_3/Tutorial_3_Coding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p40k9rCLQEkF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "%matplotlib inline\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBA4Ke9hQEkG"
   },
   "source": [
    "## Exercise 1\n",
    "### Regression\n",
    "#### Build a regression model to estimate the weight based on height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPSzzT72QEkH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pandas import read_csv, get_dummies\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Determine the environment\n",
    "is_colab = 'google.colab' in sys.modules\n",
    "# # Load the dataset conditionally\n",
    "if is_colab:\n",
    "    # Code for Google Colab environment\n",
    "    df = read_csv(\"https://raw.githubusercontent.com/wooihaw/ERA3036_T2310/main/data/genders_heights_weights.csv\")\n",
    "else:\n",
    "    # Code for local Jupyter Notebook environment\n",
    "    df = read_csv(\"../data/genders_heights_weights.csv\")\n",
    "\n",
    "X1 = df.drop(columns=['Gender', 'Weight'])\n",
    "y1 = df['Weight']\n",
    "X1_train, X1_test, y1_train, y1_test = split(X1, y1, train_size=0.75, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qu_OGKZRQEkH"
   },
   "source": [
    "To do:\n",
    "- Build a linear regression model (name it as lnr1) to estimate weight using height\n",
    "- Evaluate the model's performance with R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgpJVmGLQEkH",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HxwPao4QEkI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = eval(input('Enter height: '))\n",
    "print(f'Estimated weight is: {lnr1.predict([[h]])}kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVbMvaipQEkI"
   },
   "source": [
    "To do:\n",
    "- Apply categorical encoding using the get_dummies() function from pandas\n",
    "- Print 5 random data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcLxWG6iQEkI",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaRB6jsuQEkI"
   },
   "source": [
    "To do:\n",
    "- Separate into fetures and targets\n",
    "- Split to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89EssYpWQEkI",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBnQUAxVQEkI"
   },
   "source": [
    "To do:\n",
    "- Build another linear regression model to estimate the weight based on height and both genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggmvKTwBQEkI",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PTQAR6U8QEkI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h = eval(input('Enter height: '))\n",
    "g = eval(input('Enter gender (0 for male, 1 for female): '))\n",
    "print(f'Estimated weight is: {lnr2.predict([[h, g, 1-g]])}kg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p87LwoSIQEkJ"
   },
   "source": [
    "## Exercise 2\n",
    "### Classification\n",
    "#### Compare 7 classification models for handwritten digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZhE6aHYQEkJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split as split, KFold, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "# View one of the image\n",
    "plt.imshow(X[0, :].reshape(8, 8), cmap='gray')\n",
    "plt.axis(False)\n",
    "plt.title(f'Digit: {y[0]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAudg4m4QEkJ"
   },
   "source": [
    "To do:\n",
    "- Check the number of features in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za4nhp5AQEkJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L8oABD9QEkJ"
   },
   "source": [
    "To do:\n",
    "- Use spot-checking technique to compare 7 classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/spot-check-machine-learning-algorithms-in-python/\n",
    "1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMdcKbf5QEkJ"
   },
   "outputs": [],
   "source": [
    "# load the dataset, returns X and y elements\n",
    "def load_dataset():\n",
    "    X, y = None, None\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "    # ...\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a feature preparation pipeline for a model\n",
    "def make_pipeline(model):\n",
    "    steps = list()\n",
    "    # standardization\n",
    "    steps.append(('standardize', StandardScaler()))\n",
    "    # normalization\n",
    "    steps.append(('normalize', MinMaxScaler()))\n",
    "    # the model\n",
    "    steps.append(('model', model))\n",
    "    # create pipeline\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single model\n",
    "def evaluate_model(X, y, model, folds, metric):\n",
    "    # create the pipeline\n",
    "    pipeline = make_pipeline(model)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=metric, cv=folds, n_jobs=-1)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X, y, model, folds, metric):\n",
    "    scores = None\n",
    "    try:\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            scores = evaluate_model(X, y, model, folds, metric)\n",
    "    except:\n",
    "        scores = None\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate a dictionary of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X, y, models, folds=10, metric='accuracy'):\n",
    "    results = dict()\n",
    "    for name, model in models.items():\n",
    "        # evaluate the model\n",
    "        scores = robust_evaluate_model(X, y, model, folds, metric)\n",
    "        # show process\n",
    "        if scores is not None:\n",
    "            # store a result\n",
    "            results[name] = scores\n",
    "            mean_score, std_score = mean(scores), std(scores)\n",
    "            print('>%s: %.3f (+/-%.3f)' % (name, mean_score, std_score))\n",
    "        else:\n",
    "            print('>%s: error' % name)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=10):\n",
    "    # check for no results\n",
    "    if len(results) == 0:\n",
    "        print('no results')\n",
    "        return\n",
    "    # determine how many results to summarize\n",
    "    n = min(top_n, len(results))\n",
    "    # create a list of (name, mean(scores)) tuples\n",
    "    mean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "    # sort tuples by mean score\n",
    "    mean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "    # reverse for descending order (e.g. for accuracy)\n",
    "    if maximize:\n",
    "        mean_scores = list(reversed(mean_scores))\n",
    "    # retrieve the top n for summarization\n",
    "    names = [x[0] for x in mean_scores[:n]]\n",
    "    scores = [results[x[0]] for x in mean_scores[:n]]\n",
    "    # print the top n\n",
    "    print()\n",
    "    for i in range(n):\n",
    "        name = names[i]\n",
    "        mean_score, std_score = mean(results[name]), std(results[name])\n",
    "        print('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "    # boxplot for the top n\n",
    "    pyplot.boxplot(scores, labels=names)\n",
    "    _, labels = pyplot.xticks()\n",
    "    pyplot.setp(labels, rotation=90)\n",
    "    pyplot.savefig('spotcheck.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aiM0Nk1cQEkJ"
   },
   "source": [
    "To do:\n",
    "- Use Univariate Selection to select 20 best features. Evaluate the performance of the best model above on these features using 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMtIN1VOQEkJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
